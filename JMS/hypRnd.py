# RandomizedSearchCV
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor

def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

# === ë°ì´í„° ë¡œë“œ ===
df = pd.read_csv("data.csv", encoding="cp949")  # í•„ìš” ì‹œ ì‹ ì…ìƒ íŒŒì¼ë¡œ ë³€ê²½
target = "ì¤‘ë„íƒˆë½ìœ¨" # ì¤‘ë„íƒˆë½í•™ìƒ(ì‹ ì…ìƒ)ë¹„ìœ¨(%) / ì¤‘ë„íƒˆë½ìœ¨

features = [
    "ì •ì›ë‚´ ì‹ ì…ìƒ ì¶©ì›ìœ¨(%)", "ê¸°ìˆ™ì‚¬ ìˆ˜ìš©ë¥ ", "1ì¸ë‹¹ì¥í•™ê¸ˆ", "ì†Œê·œëª¨ ê°•ì¢Œ ë¹„ìœ¨",
    "ì „ì„êµì› 1ì¸ë‹¹ í•™ìƒìˆ˜(ì •ì›ê¸°ì¤€)", "ì·¨ì—…ë¥ ", "ì§„í•™ë¥ ", "ê°€ì¤‘ í‰ê·  ì¬í•™ìƒ ì¶©ì›ìœ¨(%)"
]

df_clean = df.dropna(subset=features + [target])
X = df_clean[features]
y = df_clean[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# === íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„í¬ ì •ì˜
param_dist = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [3, 5, 7, 9],
    'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.2, 0.3],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

# === RandomizedSearchCV ì‹¤í–‰
random_search = RandomizedSearchCV(
    estimator=XGBRegressor(tree_method='hist', random_state=42),
    param_distributions=param_dist,
    n_iter=30,                      # ë¬´ì‘ìœ„ 30ì¡°í•©ë§Œ íƒìƒ‰
    scoring='neg_root_mean_squared_error',
    cv=5,
    n_jobs=-1,
    verbose=1
)

random_search.fit(X_train, y_train)

# === ê²°ê³¼ ì¶œë ¥
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)

print("\nğŸ“Œ Best Params:", random_search.best_params_)
print(f"âœ… ìµœì  ëª¨ë¸ RMSE: {rmse(y_test, y_pred):.4f}")
print(f"âœ… ìµœì  ëª¨ë¸ RÂ²: {r2_score(y_test, y_pred):.4f}")
